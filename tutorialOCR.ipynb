{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/techwithanirudh/Automatic-Image-Captioning/blob/master/tutorialOCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP-v0E_S-mQP"
      },
      "source": [
        "<img src=\"https://github.com/arthurflor23/handwritten-text-recognition/blob/master/doc/image/header.png?raw=true\" />\n",
        "\n",
        "# Handwritten Text Recognition using TensorFlow 2.x\n",
        "\n",
        "This tutorial shows how you can use the project [Handwritten Text Recognition](https://github.com/arthurflor23/handwritten-text-recognition) in your Google Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3R4T1X05Xoi"
      },
      "source": [
        "!git clone https://github.com/arthurflor23/handwritten-text-recognition\r\n",
        "!mkdir /content/handwritten-text-recognition/raw/bentham\r\n",
        "!wget http://www.transcriptorium.eu/~tsdata/BenthamR0/BenthamDatasetR0-GT.zip\r\n",
        "!wget http://www.transcriptorium.eu/~tsdata/BenthamR0/BenthamDatasetR0-Images.zip\r\n",
        "!unzip /content/BenthamDatasetR0-GT.zip -d /content/handwritten-text-recognition/raw/bentham\r\n",
        "!unzip /content/BenthamDatasetR0-Images.zip -d /content/handwritten-text-recognition/raw/bentham\r\n",
        "\r\n",
        "# import shutil\r\n",
        "\r\n",
        "# shutil.rmtree('/content/handwritten-text-recognition/raw/bentham')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef0zWrGUNTsp"
      },
      "source": [
        "\"\"\"\r\n",
        "Provides options via the command line to perform project tasks.\r\n",
        "* `--source`: dataset/model name (bentham, iam, rimes, saintgall, washington)\r\n",
        "* `--arch`: network to be used (puigcerver, bluche, flor)\r\n",
        "* `--transform`: transform dataset to the HDF5 file\r\n",
        "* `--cv2`: visualize sample from transformed dataset\r\n",
        "* `--kaldi_assets`: save all assets for use with kaldi\r\n",
        "* `--image`: predict a single image with the source parameter\r\n",
        "* `--train`: train model with the source argument\r\n",
        "* `--test`: evaluate and predict model with the source argument\r\n",
        "* `--norm_accentuation`: discard accentuation marks in the evaluation\r\n",
        "* `--norm_punctuation`: discard punctuation marks in the evaluation\r\n",
        "* `--epochs`: number of epochs\r\n",
        "* `--batch_size`: number of batches\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "import argparse\r\n",
        "import cv2\r\n",
        "import h5py\r\n",
        "import os\r\n",
        "import string\r\n",
        "import datetime\r\n",
        "\r\n",
        "from data import preproc as pp, evaluation\r\n",
        "from data.generator import DataGenerator, Tokenizer\r\n",
        "from data.reader import Dataset\r\n",
        "from network.model import HTRModel\r\n",
        "import os\r\n",
        "\r\n",
        "os.chdir('/content/handwritten-text-recognition')\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    parser = argparse.ArgumentParser()\r\n",
        "    parser.add_argument(\"--source\", type=str, default=\"bentham\")\r\n",
        "    parser.add_argument(\"--arch\", type=str, default=\"flor\")\r\n",
        "\r\n",
        "    parser.add_argument(\"--transform\", action=\"store_true\", default=False)\r\n",
        "    parser.add_argument(\"--cv2\", action=\"store_true\", default=False)\r\n",
        "    parser.add_argument(\"--image\", type=str, default=\"\")\r\n",
        "    parser.add_argument(\"--kaldi_assets\", action=\"store_true\", default=False)\r\n",
        "\r\n",
        "    parser.add_argument(\"--train\", action=\"store_true\", default=False)\r\n",
        "    parser.add_argument(\"--test\", action=\"store_true\", default=False)\r\n",
        "\r\n",
        "    parser.add_argument(\"--norm_accentuation\", action=\"store_true\", default=False)\r\n",
        "    parser.add_argument(\"--norm_punctuation\", action=\"store_true\", default=False)\r\n",
        "\r\n",
        "    parser.add_argument(\"--epochs\", type=int, default=1000)\r\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=16)\r\n",
        "    args = parser.parse_args()\r\n",
        "\r\n",
        "    raw_path = os.path.join(\"./\", \"raw\", args.source)\r\n",
        "    source_path = os.path.join(\"./\", \"data\", f\"{args.source}.hdf5\")\r\n",
        "    output_path = os.path.join(\"./\", \"output\", args.source, args.arch)\r\n",
        "    target_path = os.path.join(output_path, \"checkpoint_weights.hdf5\")\r\n",
        "\r\n",
        "    input_size = (1024, 128, 1)\r\n",
        "    max_text_length = 128\r\n",
        "    charset_base = string.printable[:95]\r\n",
        "\r\n",
        "    if args.transform:\r\n",
        "        print(f\"{args.source} dataset will be transformed...\")\r\n",
        "        ds = Dataset(source=raw_path, name=args.source)\r\n",
        "        ds.read_partitions()\r\n",
        "        ds.save_partitions(source_path, input_size, max_text_length)\r\n",
        "\r\n",
        "    elif args.cv2:\r\n",
        "        with h5py.File(source_path, \"r\") as hf:\r\n",
        "            dt = hf['test']['dt'][:256]\r\n",
        "            gt = hf['test']['gt'][:256]\r\n",
        "\r\n",
        "        predict_file = os.path.join(output_path, \"predict.txt\")\r\n",
        "        predicts = [''] * len(dt)\r\n",
        "\r\n",
        "        if os.path.isfile(predict_file):\r\n",
        "            with open(predict_file, \"r\") as lg:\r\n",
        "                predicts = [line[5:] for line in lg if line.startswith(\"TE_P\")]\r\n",
        "\r\n",
        "        for x in range(len(dt)):\r\n",
        "            print(f\"Image shape:\\t{dt[x].shape}\")\r\n",
        "            print(f\"Ground truth:\\t{gt[x].decode()}\")\r\n",
        "            print(f\"Predict:\\t{predicts[x]}\\n\")\r\n",
        "\r\n",
        "            cv2.imshow(\"img\", pp.adjust_to_see(dt[x]))\r\n",
        "            cv2.waitKey(0)\r\n",
        "\r\n",
        "    elif args.image:\r\n",
        "        tokenizer = Tokenizer(chars=charset_base, max_text_length=max_text_length)\r\n",
        "\r\n",
        "        img = pp.preprocess(args.image, input_size=input_size)\r\n",
        "        x_test = pp.normalization([img])\r\n",
        "\r\n",
        "        model = HTRModel(architecture=args.arch,\r\n",
        "                         input_size=input_size,\r\n",
        "                         vocab_size=tokenizer.vocab_size,\r\n",
        "                         beam_width=10,\r\n",
        "                         top_paths=10)\r\n",
        "\r\n",
        "        model.compile(learning_rate=0.001)\r\n",
        "        model.load_checkpoint(target=target_path)\r\n",
        "\r\n",
        "        predicts, probabilities = model.predict(x_test, ctc_decode=True)\r\n",
        "        predicts = [[tokenizer.decode(x) for x in y] for y in predicts]\r\n",
        "\r\n",
        "        print(\"\\n####################################\")\r\n",
        "        for i, (pred, prob) in enumerate(zip(predicts, probabilities)):\r\n",
        "            print(\"\\nProb.  - Predict\")\r\n",
        "\r\n",
        "            for (pd, pb) in zip(pred, prob):\r\n",
        "                print(f\"{pb:.4f} - {pd}\")\r\n",
        "\r\n",
        "            cv2.imshow(f\"Image {i + 1}\", cv2.imread(args.image))\r\n",
        "        print(\"\\n####################################\")\r\n",
        "        cv2.waitKey(0)\r\n",
        "\r\n",
        "    else:\r\n",
        "        assert os.path.isfile(source_path) or os.path.isfile(target_path)\r\n",
        "        os.makedirs(output_path, exist_ok=True)\r\n",
        "\r\n",
        "        dtgen = DataGenerator(source=source_path,\r\n",
        "                              batch_size=args.batch_size,\r\n",
        "                              charset=charset_base,\r\n",
        "                              max_text_length=max_text_length,\r\n",
        "                              predict=(not args.kaldi_assets) and args.test)\r\n",
        "\r\n",
        "        model = HTRModel(architecture=args.arch,\r\n",
        "                         input_size=input_size,\r\n",
        "                         vocab_size=dtgen.tokenizer.vocab_size,\r\n",
        "                         beam_width=10,\r\n",
        "                         stop_tolerance=20,\r\n",
        "                         reduce_tolerance=15)\r\n",
        "\r\n",
        "        model.compile(learning_rate=0.001)\r\n",
        "        model.load_checkpoint(target=target_path)\r\n",
        "\r\n",
        "        if args.kaldi_assets:\r\n",
        "            predicts, _ = model.predict(x=dtgen.next_test_batch(), steps=dtgen.steps['test'], ctc_decode=False)\r\n",
        "            pp.generate_kaldi_assets(output_path, dtgen, predicts)\r\n",
        "\r\n",
        "        elif args.train:\r\n",
        "            model.summary(output_path, \"summary.txt\")\r\n",
        "            callbacks = model.get_callbacks(logdir=output_path, checkpoint=target_path, verbose=1)\r\n",
        "\r\n",
        "            start_time = datetime.datetime.now()\r\n",
        "\r\n",
        "            h = model.fit(x=dtgen.next_train_batch(),\r\n",
        "                          epochs=args.epochs,\r\n",
        "                          steps_per_epoch=dtgen.steps['train'],\r\n",
        "                          validation_data=dtgen.next_valid_batch(),\r\n",
        "                          validation_steps=dtgen.steps['valid'],\r\n",
        "                          callbacks=callbacks,\r\n",
        "                          shuffle=True,\r\n",
        "                          verbose=1)\r\n",
        "\r\n",
        "            total_time = datetime.datetime.now() - start_time\r\n",
        "\r\n",
        "            loss = h.history['loss']\r\n",
        "            val_loss = h.history['val_loss']\r\n",
        "\r\n",
        "            min_val_loss = min(val_loss)\r\n",
        "            min_val_loss_i = val_loss.index(min_val_loss)\r\n",
        "\r\n",
        "            time_epoch = (total_time / len(loss))\r\n",
        "            total_item = (dtgen.size['train'] + dtgen.size['valid'])\r\n",
        "\r\n",
        "            t_corpus = \"\\n\".join([\r\n",
        "                f\"Total train images:      {dtgen.size['train']}\",\r\n",
        "                f\"Total validation images: {dtgen.size['valid']}\",\r\n",
        "                f\"Batch:                   {dtgen.batch_size}\\n\",\r\n",
        "                f\"Total time:              {total_time}\",\r\n",
        "                f\"Time per epoch:          {time_epoch}\",\r\n",
        "                f\"Time per item:           {time_epoch / total_item}\\n\",\r\n",
        "                f\"Total epochs:            {len(loss)}\",\r\n",
        "                f\"Best epoch               {min_val_loss_i + 1}\\n\",\r\n",
        "                f\"Training loss:           {loss[min_val_loss_i]:.8f}\",\r\n",
        "                f\"Validation loss:         {min_val_loss:.8f}\"\r\n",
        "            ])\r\n",
        "\r\n",
        "            with open(os.path.join(output_path, \"train.txt\"), \"w\") as lg:\r\n",
        "                lg.write(t_corpus)\r\n",
        "                print(t_corpus)\r\n",
        "\r\n",
        "        elif args.test:\r\n",
        "            start_time = datetime.datetime.now()\r\n",
        "\r\n",
        "            predicts, _ = model.predict(x=dtgen.next_test_batch(),\r\n",
        "                                        steps=dtgen.steps['test'],\r\n",
        "                                        ctc_decode=True,\r\n",
        "                                        verbose=1)\r\n",
        "\r\n",
        "            predicts = [dtgen.tokenizer.decode(x[0]) for x in predicts]\r\n",
        "            ground_truth = [x.decode() for x in dtgen.dataset['test']['gt']]\r\n",
        "\r\n",
        "            total_time = datetime.datetime.now() - start_time\r\n",
        "\r\n",
        "            with open(os.path.join(output_path, \"predict.txt\"), \"w\") as lg:\r\n",
        "                for pd, gt in zip(predicts, ground_truth):\r\n",
        "                    lg.write(f\"TE_L {gt}\\nTE_P {pd}\\n\")\r\n",
        "\r\n",
        "            evaluate = evaluation.ocr_metrics(predicts=predicts,\r\n",
        "                                              ground_truth=ground_truth,\r\n",
        "                                              norm_accentuation=args.norm_accentuation,\r\n",
        "                                              norm_punctuation=args.norm_punctuation)\r\n",
        "\r\n",
        "            e_corpus = \"\\n\".join([\r\n",
        "                f\"Total test images:    {dtgen.size['test']}\",\r\n",
        "                f\"Total time:           {total_time}\",\r\n",
        "                f\"Time per item:        {total_time / dtgen.size['test']}\\n\",\r\n",
        "                f\"Metrics:\",\r\n",
        "                f\"Character Error Rate: {evaluate[0]:.8f}\",\r\n",
        "                f\"Word Error Rate:      {evaluate[1]:.8f}\",\r\n",
        "                f\"Sequence Error Rate:  {evaluate[2]:.8f}\"\r\n",
        "            ])\r\n",
        "\r\n",
        "            sufix = (\"_norm\" if args.norm_accentuation or args.norm_punctuation else \"\") + \\\r\n",
        "                    (\"_accentuation\" if args.norm_accentuation else \"\") + \\\r\n",
        "                    (\"_punctuation\" if args.norm_punctuation else \"\")\r\n",
        "\r\n",
        "            with open(os.path.join(output_path, f\"evaluate{sufix}.txt\"), \"w\") as lg:\r\n",
        "                lg.write(e_corpus)\r\n",
        "                print(e_corpus)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjoXlGji-EFO",
        "outputId": "5db2dfaa-3e48-4714-b78d-4e33cba09bfa"
      },
      "source": [
        "# !apt-get install python3-venv\r\n",
        "# !python -m venv .venv && source .venv/bin/activate\r\n",
        "# !pip install -r /content/handwritten-text-recognition/requirements.txt\r\n",
        "# !python /content/handwritten-text-recognition/src/main.py  --transform"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bentham dataset will be transformed...\n",
            "tcmalloc: large alloc 1154482176 bytes == 0x55d2000 @  0x7fc635832001 0x7fc629af74ff 0x7fc629b47b08 0x7fc629b4bac7 0x7fc629bea1a3 0x50a4a5 0x50cc96 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x50ad03 0x634e72 0x634f27 0x6386df 0x639281 0x4b0dc0 0x7fc63542dbf7 0x5b259a\n",
            "12288it [05:22, 38.12it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMty1YwuWHpN"
      },
      "source": [
        "## 1 Localhost Environment\n",
        "<!--  -->\n",
        "We'll make sure you have the project in your Google Drive with the datasets in HDF5. If you already have structured files in the cloud, skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39blvPTPQJpt"
      },
      "source": [
        "### 1.1 Datasets\n",
        "\n",
        "The datasets that you can use:\n",
        "\n",
        "a. [Bentham](http://transcriptorium.eu/datasets/bentham-collection/)\n",
        "\n",
        "b. [IAM](http://www.fki.inf.unibe.ch/databases/iam-handwriting-database)\n",
        "\n",
        "c. [Rimes](http://www.a2ialab.com/doku.php?id=rimes_database:start)\n",
        "\n",
        "d. [Saint Gall](http://www.fki.inf.unibe.ch/databases/iam-historical-document-database/saint-gall-database)\n",
        "\n",
        "e. [Washington](http://www.fki.inf.unibe.ch/databases/iam-historical-document-database/washington-database)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVBGMLifWQwl"
      },
      "source": [
        "### 1.2 Raw folder\n",
        "\n",
        "On localhost, download the code project from GitHub and extract the chosen dataset (or all if you prefer) in the **raw** folder. Don't change anything of the structure of the dataset, since the scripts were made from the **original structure** of them. Your project directory will be like this:\n",
        "\n",
        "```\n",
        ".\n",
        "├── raw\n",
        "│   ├── bentham\n",
        "│   │   ├── BenthamDatasetR0-GT\n",
        "│   │   └── BenthamDatasetR0-Images\n",
        "│   ├── iam\n",
        "│   │   ├── ascii\n",
        "│   │   ├── forms\n",
        "│   │   ├── largeWriterIndependentTextLineRecognitionTask\n",
        "│   │   ├── lines\n",
        "│   │   └── xml\n",
        "│   ├── rimes\n",
        "│   │   ├── eval_2011\n",
        "│   │   ├── eval_2011_annotated.xml\n",
        "│   │   ├── training_2011\n",
        "│   │   └── training_2011.xml\n",
        "│   ├── saintgall\n",
        "│   │   ├── data\n",
        "│   │   ├── ground_truth\n",
        "│   │   ├── README.txt\n",
        "│   │   └── sets\n",
        "│   └── washington\n",
        "│       ├── data\n",
        "│       ├── ground_truth\n",
        "│       ├── README.txt\n",
        "│       └── sets\n",
        "└── src\n",
        "    ├── data\n",
        "    │   ├── evaluation.py\n",
        "    │   ├── generator.py\n",
        "    │   ├── preproc.py\n",
        "    │   ├── reader.py\n",
        "    │   ├── similar_error_analysis.py\n",
        "    ├── main.py\n",
        "    ├── network\n",
        "    │   ├── architecture.py\n",
        "    │   ├── layers.py\n",
        "    │   ├── model.py\n",
        "    └── tutorial.ipynb\n",
        "\n",
        "```\n",
        "\n",
        "After that, create virtual environment and install the dependencies with python 3 and pip:\n",
        "\n",
        "> ```python -m venv .venv && source .venv/bin/activate```\n",
        "\n",
        "> ```pip install -r requirements.txt```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyLRbAwsWSYA"
      },
      "source": [
        "### 1.3 HDF5 files\n",
        "\n",
        "Now, you'll run the *transform* function from **main.py**. For this, execute on **src** folder:\n",
        "\n",
        "> ```python main.py --source=<DATASET_NAME> --transform```\n",
        "\n",
        "Your data will be preprocess and encode, creating and saving in the **data** folder. Now your project directory will be like this:\n",
        "\n",
        "\n",
        "```\n",
        ".\n",
        "├── data\n",
        "│   ├── bentham.hdf5\n",
        "│   ├── iam.hdf5\n",
        "│   ├── rimes.hdf5\n",
        "│   ├── saintgall.hdf5\n",
        "│   └── washington.hdf5\n",
        "├── raw\n",
        "│   ├── bentham\n",
        "│   │   ├── BenthamDatasetR0-GT\n",
        "│   │   └── BenthamDatasetR0-Images\n",
        "│   ├── iam\n",
        "│   │   ├── ascii\n",
        "│   │   ├── forms\n",
        "│   │   ├── largeWriterIndependentTextLineRecognitionTask\n",
        "│   │   ├── lines\n",
        "│   │   └── xml\n",
        "│   ├── rimes\n",
        "│   │   ├── eval_2011\n",
        "│   │   ├── eval_2011_annotated.xml\n",
        "│   │   ├── training_2011\n",
        "│   │   └── training_2011.xml\n",
        "│   ├── saintgall\n",
        "│   │   ├── data\n",
        "│   │   ├── ground_truth\n",
        "│   │   ├── README.txt\n",
        "│   │   └── sets\n",
        "│   └── washington\n",
        "│       ├── data\n",
        "│       ├── ground_truth\n",
        "│       ├── README.txt\n",
        "│       └── sets\n",
        "└── src\n",
        "    ├── data\n",
        "    │   ├── evaluation.py\n",
        "    │   ├── generator.py\n",
        "    │   ├── preproc.py\n",
        "    │   ├── reader.py\n",
        "    │   ├── similar_error_analysis.py\n",
        "    ├── main.py\n",
        "    ├── network\n",
        "    │   ├── architecture.py\n",
        "    │   ├── layers.py\n",
        "    │   ├── model.py\n",
        "    └── tutorial.ipynb\n",
        "\n",
        "```\n",
        "\n",
        "Then upload the **data** and **src** folders in the same directory in your Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jydsAcWgWVth"
      },
      "source": [
        "## 2 Google Drive Environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk3e7YJiXzSl"
      },
      "source": [
        "### 2.1 TensorFlow 2.x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7twXyNGXtbJ"
      },
      "source": [
        "Make sure the jupyter notebook is using GPU mode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHw4tODULT1Z"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJECz8H8XVCY"
      },
      "source": [
        "Now, we'll install and switch to TensorFlow 2.x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMg-B5PH9h3r"
      },
      "source": [
        "!pip install -q tensorflow-gpu\n",
        "\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5ukHtpZiz0g"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name != \"/device:GPU:0\":\n",
        "    raise SystemError(\"GPU device not found\")\n",
        "\n",
        "print(f\"Found GPU at: {device_name}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyMv5wyDXxqc"
      },
      "source": [
        "### 2.2 Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5gj6qwoX9W3"
      },
      "source": [
        "Mount your Google Drive partition.\n",
        "\n",
        "**Note:** *\\\"Colab Notebooks/handwritten-text-recognition/src/\\\"* was the directory where you put the project folders, specifically the **src** folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACQn1iBF9k9O"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"./gdrive\", force_remount=True)\n",
        "\n",
        "%cd \"./gdrive/My Drive/Colab Notebooks/handwritten-text-recognition/src/\"\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwogUA8RZAyp"
      },
      "source": [
        "After mount, you can see the list os files in the project folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fj7fSngY1IX"
      },
      "source": [
        "## 3 Set Python Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6Q4cOlWhNl3"
      },
      "source": [
        "### 3.1 Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvqL2Eq5ZUc7"
      },
      "source": [
        "First, let's define our environment variables.\n",
        "\n",
        "Set the main configuration parameters, like input size, batch size, number of epochs and list of characters. This make compatible with **main.py** and jupyter notebook:\n",
        "\n",
        "* **dataset**: \"bentham\", \"iam\", \"rimes\", \"saintgall\", \"washington\"\n",
        "\n",
        "* **arch**: network to run: \"bluche\", \"puigcerver\", \"flor\"\n",
        "\n",
        "* **epochs**: number of epochs\n",
        "\n",
        "* **batch_size**: number size of the batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qpr3drnGMWS"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import string\n",
        "\n",
        "# define parameters\n",
        "source = \"bentham\"\n",
        "arch = \"flor\"\n",
        "epochs = 1000\n",
        "batch_size = 16\n",
        "\n",
        "# define paths\n",
        "source_path = os.path.join(\"..\", \"data\", f\"{source}.hdf5\")\n",
        "output_path = os.path.join(\"..\", \"output\", source, arch)\n",
        "target_path = os.path.join(output_path, \"checkpoint_weights.hdf5\")\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# define input size, number max of chars per line and list of valid chars\n",
        "input_size = (1024, 128, 1)\n",
        "max_text_length = 128\n",
        "charset_base = string.printable[:95]\n",
        "\n",
        "print(\"source:\", source_path)\n",
        "print(\"output\", output_path)\n",
        "print(\"target\", target_path)\n",
        "print(\"charset:\", charset_base)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFextshOhTKr"
      },
      "source": [
        "### 3.2 DataGenerator Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfZ1mfvsanu1"
      },
      "source": [
        "The second class is **DataGenerator()**, responsible for:\n",
        "\n",
        "* Load the dataset partitions (train, valid, test);\n",
        "\n",
        "* Manager batchs for train/validation/test process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k9vpNzMIAi2"
      },
      "source": [
        "from data.generator import DataGenerator\n",
        "\n",
        "dtgen = DataGenerator(source=source_path,\n",
        "                      batch_size=batch_size,\n",
        "                      charset=charset_base,\n",
        "                      max_text_length=max_text_length)\n",
        "\n",
        "print(f\"Train images: {dtgen.size['train']}\")\n",
        "print(f\"Validation images: {dtgen.size['valid']}\")\n",
        "print(f\"Test images: {dtgen.size['test']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OdgNLK0hYAA"
      },
      "source": [
        "### 3.3 HTRModel Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHktk8AFcnKy"
      },
      "source": [
        "The third class is **HTRModel()**, was developed to be easy to use and to abstract the complicated flow of a HTR system. It's responsible for:\n",
        "\n",
        "* Create model with Handwritten Text Recognition flow, in which calculate the loss function by CTC and decode output to calculate the HTR metrics (CER, WER and SER);\n",
        "\n",
        "* Save and load model;\n",
        "\n",
        "* Load weights in the models (train/infer);\n",
        "\n",
        "* Make Train/Predict process using *generator*.\n",
        "\n",
        "To make a dynamic HTRModel, its parameters are the *architecture*, *input_size* and *vocab_size*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV0GreStISTR"
      },
      "source": [
        "from network.model import HTRModel\n",
        "\n",
        "# create and compile HTRModel\n",
        "model = HTRModel(architecture=arch,\n",
        "                 input_size=input_size,\n",
        "                 vocab_size=dtgen.tokenizer.vocab_size,\n",
        "                 beam_width=10,\n",
        "                 stop_tolerance=20,\n",
        "                 reduce_tolerance=15)\n",
        "\n",
        "model.compile(learning_rate=0.001)\n",
        "model.summary(output_path, \"summary.txt\")\n",
        "\n",
        "# get default callbacks and load checkpoint weights file (HDF5) if exists\n",
        "model.load_checkpoint(target=target_path)\n",
        "\n",
        "callbacks = model.get_callbacks(logdir=output_path, checkpoint=target_path, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1fnz0Eugqru"
      },
      "source": [
        "## 4 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1mLOcqYgsO-"
      },
      "source": [
        "The training process is similar to the *fit()* of the Keras. After training, the information (epochs and minimum loss) is save."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P6MSoxCISlD"
      },
      "source": [
        "# to calculate total and average time per epoch\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "h = model.fit(x=dtgen.next_train_batch(),\n",
        "              epochs=epochs,\n",
        "              steps_per_epoch=dtgen.steps['train'],\n",
        "              validation_data=dtgen.next_valid_batch(),\n",
        "              validation_steps=dtgen.steps['valid'],\n",
        "              callbacks=callbacks,\n",
        "              shuffle=True,\n",
        "              verbose=1)\n",
        "\n",
        "total_time = datetime.datetime.now() - start_time\n",
        "\n",
        "loss = h.history['loss']\n",
        "val_loss = h.history['val_loss']\n",
        "\n",
        "min_val_loss = min(val_loss)\n",
        "min_val_loss_i = val_loss.index(min_val_loss)\n",
        "\n",
        "time_epoch = (total_time / len(loss))\n",
        "total_item = (dtgen.size['train'] + dtgen.size['valid'])\n",
        "\n",
        "t_corpus = \"\\n\".join([\n",
        "    f\"Total train images:      {dtgen.size['train']}\",\n",
        "    f\"Total validation images: {dtgen.size['valid']}\",\n",
        "    f\"Batch:                   {dtgen.batch_size}\\n\",\n",
        "    f\"Total time:              {total_time}\",\n",
        "    f\"Time per epoch:          {time_epoch}\",\n",
        "    f\"Time per item:           {time_epoch / total_item}\\n\",\n",
        "    f\"Total epochs:            {len(loss)}\",\n",
        "    f\"Best epoch               {min_val_loss_i + 1}\\n\",\n",
        "    f\"Training loss:           {loss[min_val_loss_i]:.8f}\",\n",
        "    f\"Validation loss:         {min_val_loss:.8f}\"\n",
        "])\n",
        "\n",
        "with open(os.path.join(output_path, \"train.txt\"), \"w\") as lg:\n",
        "    lg.write(t_corpus)\n",
        "    print(t_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13g7tDjWgtXV"
      },
      "source": [
        "## 5 Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddO26OT-g_QK"
      },
      "source": [
        "The predict process is similar to the *predict* of the Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9iHL6tmaL_j"
      },
      "source": [
        "from data import preproc as pp\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "# predict() function will return the predicts with the probabilities\n",
        "predicts, _ = model.predict(x=dtgen.next_test_batch(),\n",
        "                            steps=dtgen.steps['test'],\n",
        "                            ctc_decode=True,\n",
        "                            verbose=1)\n",
        "\n",
        "# decode to string\n",
        "predicts = [dtgen.tokenizer.decode(x[0]) for x in predicts]\n",
        "ground_truth = [x.decode() for x in dtgen.dataset['test']['gt']]\n",
        "\n",
        "total_time = datetime.datetime.now() - start_time\n",
        "\n",
        "# mount predict corpus file\n",
        "with open(os.path.join(output_path, \"predict.txt\"), \"w\") as lg:\n",
        "    for pd, gt in zip(predicts, ground_truth):\n",
        "        lg.write(f\"TE_L {gt}\\nTE_P {pd}\\n\")\n",
        "   \n",
        "for i, item in enumerate(dtgen.dataset['test']['dt'][:10]):\n",
        "    print(\"=\" * 1024, \"\\n\")\n",
        "    cv2_imshow(pp.adjust_to_see(item))\n",
        "    print(ground_truth[i])\n",
        "    print(predicts[i], \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JcAs3Q3WNJ-"
      },
      "source": [
        "## 6 Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LuZBRepWbom"
      },
      "source": [
        "Evaluation process is more manual process. Here we have the `ocr_metrics`, but feel free to implement other metrics instead. In the function, we have three parameters: \n",
        "\n",
        "* predicts\n",
        "* ground_truth\n",
        "* norm_accentuation (calculation with/without accentuation)\n",
        "* norm_punctuation (calculation with/without punctuation marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gCwEYdKWOPK"
      },
      "source": [
        "from data import evaluation\n",
        "\n",
        "evaluate = evaluation.ocr_metrics(predicts, ground_truth)\n",
        "\n",
        "e_corpus = \"\\n\".join([\n",
        "    f\"Total test images:    {dtgen.size['test']}\",\n",
        "    f\"Total time:           {total_time}\",\n",
        "    f\"Time per item:        {total_time / dtgen.size['test']}\\n\",\n",
        "    f\"Metrics:\",\n",
        "    f\"Character Error Rate: {evaluate[0]:.8f}\",\n",
        "    f\"Word Error Rate:      {evaluate[1]:.8f}\",\n",
        "    f\"Sequence Error Rate:  {evaluate[2]:.8f}\"\n",
        "])\n",
        "\n",
        "with open(os.path.join(output_path, \"evaluate.txt\"), \"w\") as lg:\n",
        "    lg.write(e_corpus)\n",
        "    print(e_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}