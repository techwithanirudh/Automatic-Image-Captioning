{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "Python OCR adv",
      "provenance": [],
      "collapsed_sections": [
        "oMty1YwuWHpN"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/techwithanirudh/Automatic-Image-Captioning/blob/master/Python_OCR_adv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP-v0E_S-mQP"
      },
      "source": [
        "<img src=\"https://github.com/arthurflor23/handwritten-text-recognition/blob/master/doc/image/header.png?raw=true\" />\n",
        "\n",
        "# Handwritten Text Recognition using TensorFlow 2.x\n",
        "\n",
        "This tutorial shows how you can use the project [Handwritten Text Recognition](https://github.com/arthurflor23/handwritten-text-recognition) in your Google Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3R4T1X05Xoi"
      },
      "source": [
        "# from google.colab import drive\r\n",
        "\r\n",
        "# drive.mount(\"./gdrive\", force_remount=True)\r\n",
        "%mkdir \"/content/gdrive/\"\r\n",
        "%mkdir \"/content/gdrive/My Drive/\"\r\n",
        "%cd \"/content/gdrive/My Drive/\"\r\n",
        "!git clone https://github.com/arthurflor23/handwritten-text-recognition\r\n",
        "!mkdir \"/content/gdrive/My Drive/handwritten-text-recognition/raw/bentham\"\r\n",
        "!wget http://www.transcriptorium.eu/~tsdata/BenthamR0/BenthamDatasetR0-GT.zip\r\n",
        "!wget http://www.transcriptorium.eu/~tsdata/BenthamR0/BenthamDatasetR0-Images.zip\r\n",
        "!unzip \"/content/gdrive/My Drive/BenthamDatasetR0-GT.zip\" -d \"/content/gdrive/My Drive/handwritten-text-recognition/raw/bentham/\"\r\n",
        "!unzip \"/content/gdrive/My Drive/BenthamDatasetR0-Images.zip\" -d \"/content/gdrive/My Drive/handwritten-text-recognition/raw/bentham/\"\r\n",
        "\r\n",
        "# import shutil\r\n",
        "\r\n",
        "# shutil.rmtree('/content/gdrive/My Drive/handwritten-text-recognition')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjoXlGji-EFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91663919-4470-4620-b766-6e7bafb6d6f8"
      },
      "source": [
        "# default='bentham'\r\n",
        "\r\n",
        "# import os\r\n",
        "\r\n",
        "# os.chdir('/content/gdrive/My Drive/handwritten-text-recognition')\r\n",
        "\r\n",
        "    # raw_path = os.path.join(\"./\", \"raw\", args.source)\r\n",
        "    # source_path = os.path.join(\"./\", \"data\", f\"{args.source}.hdf5\")\r\n",
        "    # output_path = os.path.join(\"./\", \"output\", args.source, args.arch)\r\n",
        "    # target_path = os.path.join(output_path, \"checkpoint_weights.hdf5\")\r\n",
        "\r\n",
        "!apt-get install python3-venv\r\n",
        "!python -m venv .venv && source .venv/bin/activate\r\n",
        "!pip install -r \"/content/gdrive/My Drive/handwritten-text-recognition/requirements.txt\"\r\n",
        "!python \"/content/gdrive/My Drive/handwritten-text-recognition/src/main.py\" --transform"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bentham dataset will be transformed...\n",
            "tcmalloc: large alloc 1154482176 bytes == 0x5ed6000 @  0x7f7d4ec77001 0x7f7d42cf14ff 0x7f7d42d41b08 0x7f7d42d45ac7 0x7f7d42de41a3 0x50a4a5 0x50cc96 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x50ad03 0x634e72 0x634f27 0x6386df 0x639281 0x4b0dc0 0x7f7d4e872bf7 0x5b259a\n",
            "12288it [04:10, 49.08it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMty1YwuWHpN"
      },
      "source": [
        "## 1 Localhost Environment\n",
        "<!--  -->\n",
        "We'll make sure you have the project in your Google Drive with the datasets in HDF5. If you already have structured files in the cloud, skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39blvPTPQJpt"
      },
      "source": [
        "### 1.1 Datasets\n",
        "\n",
        "The datasets that you can use:\n",
        "\n",
        "a. [Bentham](http://transcriptorium.eu/datasets/bentham-collection/)\n",
        "\n",
        "b. [IAM](http://www.fki.inf.unibe.ch/databases/iam-handwriting-database)\n",
        "\n",
        "c. [Rimes](http://www.a2ialab.com/doku.php?id=rimes_database:start)\n",
        "\n",
        "d. [Saint Gall](http://www.fki.inf.unibe.ch/databases/iam-historical-document-database/saint-gall-database)\n",
        "\n",
        "e. [Washington](http://www.fki.inf.unibe.ch/databases/iam-historical-document-database/washington-database)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVBGMLifWQwl"
      },
      "source": [
        "### 1.2 Raw folder\n",
        "\n",
        "On localhost, download the code project from GitHub and extract the chosen dataset (or all if you prefer) in the **raw** folder. Don't change anything of the structure of the dataset, since the scripts were made from the **original structure** of them. Your project directory will be like this:\n",
        "\n",
        "```\n",
        ".\n",
        "├── raw\n",
        "│   ├── bentham\n",
        "│   │   ├── BenthamDatasetR0-GT\n",
        "│   │   └── BenthamDatasetR0-Images\n",
        "│   ├── iam\n",
        "│   │   ├── ascii\n",
        "│   │   ├── forms\n",
        "│   │   ├── largeWriterIndependentTextLineRecognitionTask\n",
        "│   │   ├── lines\n",
        "│   │   └── xml\n",
        "│   ├── rimes\n",
        "│   │   ├── eval_2011\n",
        "│   │   ├── eval_2011_annotated.xml\n",
        "│   │   ├── training_2011\n",
        "│   │   └── training_2011.xml\n",
        "│   ├── saintgall\n",
        "│   │   ├── data\n",
        "│   │   ├── ground_truth\n",
        "│   │   ├── README.txt\n",
        "│   │   └── sets\n",
        "│   └── washington\n",
        "│       ├── data\n",
        "│       ├── ground_truth\n",
        "│       ├── README.txt\n",
        "│       └── sets\n",
        "└── src\n",
        "    ├── data\n",
        "    │   ├── evaluation.py\n",
        "    │   ├── generator.py\n",
        "    │   ├── preproc.py\n",
        "    │   ├── reader.py\n",
        "    │   ├── similar_error_analysis.py\n",
        "    ├── main.py\n",
        "    ├── network\n",
        "    │   ├── architecture.py\n",
        "    │   ├── layers.py\n",
        "    │   ├── model.py\n",
        "    └── tutorial.ipynb\n",
        "\n",
        "```\n",
        "\n",
        "After that, create virtual environment and install the dependencies with python 3 and pip:\n",
        "\n",
        "> ```python -m venv .venv && source .venv/bin/activate```\n",
        "\n",
        "> ```pip install -r requirements.txt```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyLRbAwsWSYA"
      },
      "source": [
        "### 1.3 HDF5 files\n",
        "\n",
        "Now, you'll run the *transform* function from **main.py**. For this, execute on **src** folder:\n",
        "\n",
        "> ```python main.py --source=<DATASET_NAME> --transform```\n",
        "\n",
        "Your data will be preprocess and encode, creating and saving in the **data** folder. Now your project directory will be like this:\n",
        "\n",
        "\n",
        "```\n",
        ".\n",
        "├── data\n",
        "│   ├── bentham.hdf5\n",
        "│   ├── iam.hdf5\n",
        "│   ├── rimes.hdf5\n",
        "│   ├── saintgall.hdf5\n",
        "│   └── washington.hdf5\n",
        "├── raw\n",
        "│   ├── bentham\n",
        "│   │   ├── BenthamDatasetR0-GT\n",
        "│   │   └── BenthamDatasetR0-Images\n",
        "│   ├── iam\n",
        "│   │   ├── ascii\n",
        "│   │   ├── forms\n",
        "│   │   ├── largeWriterIndependentTextLineRecognitionTask\n",
        "│   │   ├── lines\n",
        "│   │   └── xml\n",
        "│   ├── rimes\n",
        "│   │   ├── eval_2011\n",
        "│   │   ├── eval_2011_annotated.xml\n",
        "│   │   ├── training_2011\n",
        "│   │   └── training_2011.xml\n",
        "│   ├── saintgall\n",
        "│   │   ├── data\n",
        "│   │   ├── ground_truth\n",
        "│   │   ├── README.txt\n",
        "│   │   └── sets\n",
        "│   └── washington\n",
        "│       ├── data\n",
        "│       ├── ground_truth\n",
        "│       ├── README.txt\n",
        "│       └── sets\n",
        "└── src\n",
        "    ├── data\n",
        "    │   ├── evaluation.py\n",
        "    │   ├── generator.py\n",
        "    │   ├── preproc.py\n",
        "    │   ├── reader.py\n",
        "    │   ├── similar_error_analysis.py\n",
        "    ├── main.py\n",
        "    ├── network\n",
        "    │   ├── architecture.py\n",
        "    │   ├── layers.py\n",
        "    │   ├── model.py\n",
        "    └── tutorial.ipynb\n",
        "\n",
        "```\n",
        "\n",
        "Then upload the **data** and **src** folders in the same directory in your Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jydsAcWgWVth"
      },
      "source": [
        "## 2 Google Drive Environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk3e7YJiXzSl"
      },
      "source": [
        "### 2.1 TensorFlow 2.x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7twXyNGXtbJ"
      },
      "source": [
        "Make sure the jupyter notebook is using GPU mode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHw4tODULT1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382c1c40-7e49-4532-b61c-35ea5b5193c4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jan 27 16:10:03 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJECz8H8XVCY"
      },
      "source": [
        "Now, we'll install and switch to TensorFlow 2.x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMg-B5PH9h3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c983f80-a3fb-4748-c689-10363c33462e"
      },
      "source": [
        "!pip install -q tensorflow-gpu\n",
        "\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 394.3MB 43kB/s \n",
            "\u001b[K     |████████████████████████████████| 471kB 50.6MB/s \n",
            "\u001b[31mERROR: tensorflow 2.3.1 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.1 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 2.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5ukHtpZiz0g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "bc81864c-775d-492e-9882-66f856487092"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "if device_name != \"/device:GPU:0\":\n",
        "    raise SystemError(\"GPU device not found\")\n",
        "\n",
        "print(f\"Found GPU at: {device_name}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-a6b1f62cdd5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"/device:GPU:0\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GPU device not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Found GPU at: {device_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyMv5wyDXxqc"
      },
      "source": [
        "### 2.2 Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5gj6qwoX9W3"
      },
      "source": [
        "Mount your Google Drive partition.\n",
        "\n",
        "**Note:** *\\\"Colab Notebooks/handwritten-text-recognition/src/\\\"* was the directory where you put the project folders, specifically the **src** folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACQn1iBF9k9O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa8fc410-6eb6-4092-9b77-d1aec831c44d"
      },
      "source": [
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount(\"./gdrive\", force_remount=True)\n",
        "\n",
        "%cd '/content/'\n",
        "%cd \"./gdrive/My Drive/handwritten-text-recognition/src/\"\n",
        "!ls -l"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/gdrive/My Drive/handwritten-text-recognition/src\n",
            "total 40\n",
            "drwxr-xr-x 3 root root  4096 Jan 27 16:09 data\n",
            "-rw-r--r-- 1 root root  9074 Jan 27 15:53 main.py\n",
            "drwxr-xr-x 3 root root  4096 Jan 27 16:09 network\n",
            "-rw-r--r-- 1 root root 16473 Jan 27 15:53 tutorial.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwogUA8RZAyp"
      },
      "source": [
        "After mount, you can see the list os files in the project folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fj7fSngY1IX"
      },
      "source": [
        "## 3 Set Python Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6Q4cOlWhNl3"
      },
      "source": [
        "### 3.1 Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvqL2Eq5ZUc7"
      },
      "source": [
        "First, let's define our environment variables.\n",
        "\n",
        "Set the main configuration parameters, like input size, batch size, number of epochs and list of characters. This make compatible with **main.py** and jupyter notebook:\n",
        "\n",
        "* **dataset**: \"bentham\", \"iam\", \"rimes\", \"saintgall\", \"washington\"\n",
        "\n",
        "* **arch**: network to run: \"bluche\", \"puigcerver\", \"flor\"\n",
        "\n",
        "* **epochs**: number of epochs\n",
        "\n",
        "* **batch_size**: number size of the batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qpr3drnGMWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae5fb20a-4a2a-474d-97f2-106411cf6857"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import string\n",
        "\n",
        "\n",
        "# define parameters\n",
        "source = \"bentham\"\n",
        "arch = \"flor\"\n",
        "epochs = 1000\n",
        "batch_size = 16\n",
        "\n",
        "# define paths\n",
        "source_path = os.path.join(\"/content/gdrive/My Drive/handwritten-text-recognition/\", \"data\", f\"{source}.hdf5\")\n",
        "output_path = os.path.join(\"/content/gdrive/My Drive/handwritten-text-recognition/\", \"output\", source, arch)\n",
        "target_path = os.path.join(output_path, \"checkpoint_weights.hdf5\")\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# define input size, number max of chars per line and list of valid chars\n",
        "input_size = (1024, 128, 1)\n",
        "max_text_length = 128\n",
        "charset_base = string.printable[:95]\n",
        "\n",
        "print(\"source:\", source_path)\n",
        "print(\"output\", output_path)\n",
        "print(\"target\", target_path)\n",
        "print(\"charset:\", charset_base)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source: /content/gdrive/My Drive/handwritten-text-recognition/data/bentham.hdf5\n",
            "output /content/gdrive/My Drive/handwritten-text-recognition/output/bentham/flor\n",
            "target /content/gdrive/My Drive/handwritten-text-recognition/output/bentham/flor/checkpoint_weights.hdf5\n",
            "charset: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFextshOhTKr"
      },
      "source": [
        "### 3.2 DataGenerator Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfZ1mfvsanu1"
      },
      "source": [
        "The second class is **DataGenerator()**, responsible for:\n",
        "\n",
        "* Load the dataset partitions (train, valid, test);\n",
        "\n",
        "* Manager batchs for train/validation/test process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k9vpNzMIAi2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e37cc0f-7202-4c77-b1fc-bf61368c592c"
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content/gdrive/My Drive/handwritten-text-recognition/src/')\n",
        "\n",
        "from data.generator import DataGenerator\n",
        "\n",
        "dtgen = DataGenerator(source=source_path,\n",
        "                      batch_size=batch_size,\n",
        "                      charset=charset_base,\n",
        "                      max_text_length=max_text_length)\n",
        "\n",
        "print(f\"Train images: {dtgen.size['train']}\")\n",
        "print(f\"Validation images: {dtgen.size['valid']}\")\n",
        "print(f\"Test images: {dtgen.size['test']}\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train images: 8808\n",
            "Validation images: 1372\n",
            "Test images: 820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OdgNLK0hYAA"
      },
      "source": [
        "### 3.3 HTRModel Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHktk8AFcnKy"
      },
      "source": [
        "The third class is **HTRModel()**, was developed to be easy to use and to abstract the complicated flow of a HTR system. It's responsible for:\n",
        "\n",
        "* Create model with Handwritten Text Recognition flow, in which calculate the loss function by CTC and decode output to calculate the HTR metrics (CER, WER and SER);\n",
        "\n",
        "* Save and load model;\n",
        "\n",
        "* Load weights in the models (train/infer);\n",
        "\n",
        "* Make Train/Predict process using *generator*.\n",
        "\n",
        "To make a dynamic HTRModel, its parameters are the *architecture*, *input_size* and *vocab_size*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV0GreStISTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27b74bab-cfa0-41fb-d96c-a410756255d4"
      },
      "source": [
        "from network.model import HTRModel\n",
        "\n",
        "# create and compile HTRModel\n",
        "model = HTRModel(architecture=arch,\n",
        "                 input_size=input_size,\n",
        "                 vocab_size=dtgen.tokenizer.vocab_size,\n",
        "                 beam_width=10,\n",
        "                 stop_tolerance=20,\n",
        "                 reduce_tolerance=15)\n",
        "\n",
        "model.compile(learning_rate=0.001)\n",
        "model.summary(output_path, \"summary.txt\")\n",
        "\n",
        "# get default callbacks and load checkpoint weights file (HDF5) if exists\n",
        "model.load_checkpoint(target=target_path)\n",
        "\n",
        "callbacks = model.get_callbacks(logdir=output_path, checkpoint=target_path, verbose=1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 1024, 128, 1)]    0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 1024, 64, 16)      160       \n",
            "_________________________________________________________________\n",
            "p_re_lu (PReLU)              (None, 1024, 64, 16)      16        \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 1024, 64, 16)      112       \n",
            "_________________________________________________________________\n",
            "full_gated_conv2d (FullGated (None, 1024, 64, 16)      4640      \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 1024, 64, 32)      4640      \n",
            "_________________________________________________________________\n",
            "p_re_lu_1 (PReLU)            (None, 1024, 64, 32)      32        \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1024, 64, 32)      224       \n",
            "_________________________________________________________________\n",
            "full_gated_conv2d_1 (FullGat (None, 1024, 64, 32)      18496     \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 512, 16, 40)       10280     \n",
            "_________________________________________________________________\n",
            "p_re_lu_2 (PReLU)            (None, 512, 16, 40)       40        \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 512, 16, 40)       280       \n",
            "_________________________________________________________________\n",
            "full_gated_conv2d_2 (FullGat (None, 512, 16, 40)       28880     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512, 16, 40)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 512, 16, 48)       17328     \n",
            "_________________________________________________________________\n",
            "p_re_lu_3 (PReLU)            (None, 512, 16, 48)       48        \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 512, 16, 48)       336       \n",
            "_________________________________________________________________\n",
            "full_gated_conv2d_3 (FullGat (None, 512, 16, 48)       41568     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512, 16, 48)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 256, 4, 56)        21560     \n",
            "_________________________________________________________________\n",
            "p_re_lu_4 (PReLU)            (None, 256, 4, 56)        56        \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256, 4, 56)        392       \n",
            "_________________________________________________________________\n",
            "full_gated_conv2d_4 (FullGat (None, 256, 4, 56)        56560     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256, 4, 56)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 256, 4, 64)        32320     \n",
            "_________________________________________________________________\n",
            "p_re_lu_5 (PReLU)            (None, 256, 4, 64)        64        \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 256, 4, 64)        448       \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 256, 256)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 256, 256)          296448    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256, 256)          65792     \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 256, 256)          296448    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256, 98)           25186     \n",
            "=================================================================\n",
            "Total params: 922,354\n",
            "Trainable params: 921,074\n",
            "Non-trainable params: 1,280\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1fnz0Eugqru"
      },
      "source": [
        "## 4 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1mLOcqYgsO-"
      },
      "source": [
        "The training process is similar to the *fit()* of the Keras. After training, the information (epochs and minimum loss) is save."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P6MSoxCISlD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0f51321-a4e5-4004-a716-460e56c3e3cf"
      },
      "source": [
        "# to calculate total and average time per epoch\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "h = model.fit(x=dtgen.next_train_batch(),\n",
        "              epochs=epochs,\n",
        "              steps_per_epoch=dtgen.steps['train'],\n",
        "              validation_data=dtgen.next_valid_batch(),\n",
        "              validation_steps=dtgen.steps['valid'],\n",
        "              callbacks=callbacks,\n",
        "              shuffle=True,\n",
        "              verbose=1)\n",
        "\n",
        "total_time = datetime.datetime.now() - start_time\n",
        "\n",
        "loss = h.history['loss']\n",
        "val_loss = h.history['val_loss']\n",
        "\n",
        "min_val_loss = min(val_loss)\n",
        "min_val_loss_i = val_loss.index(min_val_loss)\n",
        "\n",
        "time_epoch = (total_time / len(loss))\n",
        "total_item = (dtgen.size['train'] + dtgen.size['valid'])\n",
        "\n",
        "t_corpus = \"\\n\".join([\n",
        "    f\"Total train images:      {dtgen.size['train']}\",\n",
        "    f\"Total validation images: {dtgen.size['valid']}\",\n",
        "    f\"Batch:                   {dtgen.batch_size}\\n\",\n",
        "    f\"Total time:              {total_time}\",\n",
        "    f\"Time per epoch:          {time_epoch}\",\n",
        "    f\"Time per item:           {time_epoch / total_item}\\n\",\n",
        "    f\"Total epochs:            {len(loss)}\",\n",
        "    f\"Best epoch               {min_val_loss_i + 1}\\n\",\n",
        "    f\"Training loss:           {loss[min_val_loss_i]:.8f}\",\n",
        "    f\"Validation loss:         {min_val_loss:.8f}\"\n",
        "])\n",
        "\n",
        "with open(os.path.join(output_path, \"train.txt\"), \"w\") as lg:\n",
        "    lg.write(t_corpus)\n",
        "    print(t_corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            " 33/551 [>.............................] - ETA: 1:28:43 - loss: 373.9944"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13g7tDjWgtXV"
      },
      "source": [
        "## 5 Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddO26OT-g_QK"
      },
      "source": [
        "The predict process is similar to the *predict* of the Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9iHL6tmaL_j"
      },
      "source": [
        "from data import preproc as pp\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "# predict() function will return the predicts with the probabilities\n",
        "predicts, _ = model.predict(x=dtgen.next_test_batch(),\n",
        "                            steps=dtgen.steps['test'],\n",
        "                            ctc_decode=True,\n",
        "                            verbose=1)\n",
        "\n",
        "# decode to string\n",
        "predicts = [dtgen.tokenizer.decode(x[0]) for x in predicts]\n",
        "ground_truth = [x.decode() for x in dtgen.dataset['test']['gt']]\n",
        "\n",
        "total_time = datetime.datetime.now() - start_time\n",
        "\n",
        "# mount predict corpus file\n",
        "with open(os.path.join(output_path, \"predict.txt\"), \"w\") as lg:\n",
        "    for pd, gt in zip(predicts, ground_truth):\n",
        "        lg.write(f\"TE_L {gt}\\nTE_P {pd}\\n\")\n",
        "   \n",
        "for i, item in enumerate(dtgen.dataset['test']['dt'][:10]):\n",
        "    print(\"=\" * 1024, \"\\n\")\n",
        "    cv2_imshow(pp.adjust_to_see(item))\n",
        "    print(ground_truth[i])\n",
        "    print(predicts[i], \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JcAs3Q3WNJ-"
      },
      "source": [
        "## 6 Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LuZBRepWbom"
      },
      "source": [
        "Evaluation process is more manual process. Here we have the `ocr_metrics`, but feel free to implement other metrics instead. In the function, we have three parameters: \n",
        "\n",
        "* predicts\n",
        "* ground_truth\n",
        "* norm_accentuation (calculation with/without accentuation)\n",
        "* norm_punctuation (calculation with/without punctuation marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gCwEYdKWOPK"
      },
      "source": [
        "from data import evaluation\n",
        "\n",
        "evaluate = evaluation.ocr_metrics(predicts, ground_truth)\n",
        "\n",
        "e_corpus = \"\\n\".join([\n",
        "    f\"Total test images:    {dtgen.size['test']}\",\n",
        "    f\"Total time:           {total_time}\",\n",
        "    f\"Time per item:        {total_time / dtgen.size['test']}\\n\",\n",
        "    f\"Metrics:\",\n",
        "    f\"Character Error Rate: {evaluate[0]:.8f}\",\n",
        "    f\"Word Error Rate:      {evaluate[1]:.8f}\",\n",
        "    f\"Sequence Error Rate:  {evaluate[2]:.8f}\"\n",
        "])\n",
        "\n",
        "with open(os.path.join(output_path, \"evaluate.txt\"), \"w\") as lg:\n",
        "    lg.write(e_corpus)\n",
        "    print(e_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}